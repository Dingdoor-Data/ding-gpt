{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph memory agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_memory_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Episode(BaseModel):\n",
    "    \"\"\"Record the agent's perspective of an interaction, capturing key internal thought processes to facilitate learning over time.\"\"\"\n",
    "    observation: str = Field(..., description=\"The context and setup - what happened\")\n",
    "    thoughts: str = Field(\n",
    "        ...,\n",
    "        description=\"Internal reasoning process and observations of the agent in the episode that led to the correct action and result. 'I ...'\",\n",
    "    )\n",
    "    action: str = Field(\n",
    "        ...,\n",
    "        description=\"What was done, how, and in what format. (Include whatever is salient to the success of the action). 'I ...'\",\n",
    "    )\n",
    "    result: str = Field(\n",
    "        ...,\n",
    "        description=\"Outcome and retrospective. What did you do well? What could you do better next time? 'I ...'\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = create_memory_manager(\n",
    "    \"gpt-4o-mini\",\n",
    "    schemas=[Episode],\n",
    "    instructions=\"Extract examples of successful explanations, capturing the full chain of reasoning. Be concise in your explanations and precise in the logic of your reasoning.\",\n",
    "    enable_inserts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractedMemory(id='575e91cb-6926-4ddf-9bac-0fcd13d7ef63', content=Episode(observation='The human asked for a definition of a binary tree, relating it to their experience with family trees.', thoughts='I need to explain the concept of a binary tree in a way that connects with their knowledge of family trees. Using a familiar analogy will help them understand the structure more effectively.', action='I compared a binary tree to a family tree by explaining that each parent can have at most two children, providing a simple diagram with Bob as the parent and Amy and Carl as children.', result='The human understood the analogy and found it relatable, confirming their comprehension.'))\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What's a binary tree? I work with family trees if that helps\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A binary tree is like a family tree, but each parent has at most 2 children. Here's a simple example:\\n   Bob\\n  /  \\\\\\n Amy  Carl\\n\\n Just like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Oh, that makes sense! So in a binary search tree, would it be like organizing a family by age?\"},\n",
    "]\n",
    "\n",
    "episodes = manager.invoke({\"messages\": conversation})\n",
    "print(episodes[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and try to store the memory in a vector database like Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'memory-db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536, \n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Episode(BaseModel):\n",
    "    observation: str = Field(..., description=\"Context of the interaction\")\n",
    "    thoughts: str = Field(..., description=\"Assistant's reasoning during the interaction\")\n",
    "    action: str = Field(..., description=\"Action taken by the assistant\")\n",
    "    result: str = Field(..., description=\"Outcome of the action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/nvnkgz9s2nnbd6d3c005qd5m0000gn/T/ipykernel_89216/3892135470.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name='gpt-4o-mini', openai_api_key='OPENAI_API_KEY')\n",
      "/var/folders/jv/nvnkgz9s2nnbd6d3c005qd5m0000gn/T/ipykernel_89216/3892135470.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(llm=llm)\n",
      "/var/folders/jv/nvnkgz9s2nnbd6d3c005qd5m0000gn/T/ipykernel_89216/3892135470.py:12: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', openai_api_key='OPENAI_API_KEY')\n",
    "\n",
    "# Initialize conversational memory\n",
    "memory = ConversationBufferMemory(llm=llm)\n",
    "\n",
    "# Create the conversation chain\n",
    "conversation = ConversationChain(llm=llm, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"user\", \"content\": \"What's a binary tree? I work with family trees if that helps.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A binary tree is like a family tree, but each parent has at most two children. Here's a simple example:\\n   Bob\\n  /  \\\\\\n Amy  Carl\\n\\nJust like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Oh, that makes sense! So in a binary search tree, would it be like organizing a family by age?\"},\n",
    "]\n",
    "\n",
    "# Generate an embedding for the conversation\n",
    "response = client.embeddings.create(\n",
    "    input=str(conversation_history),\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding\n",
    "\n",
    "# Define the episodic memory\n",
    "episode = Episode(\n",
    "    observation=\"The user asked for an explanation of a binary tree and related it to family trees.\",\n",
    "    thoughts=\"I realized the user was familiar with family trees, which allowed me to relate the concept of a binary tree to something they understood.\",\n",
    "    action=\"I explained that a binary tree consists of a parent with at most two children and illustrated it with a simple example using names.\",\n",
    "    result=\"The user expressed understanding, indicating that my explanation was clear and effective.\"\n",
    ")\n",
    "\n",
    "# Upsert the episodic memory into Pinecone\n",
    "index.upsert(vectors=[(\"unique-id-1\", embedding, episode.model_dump())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Can you explain binary trees in the context of family relationships?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "    input=user_query,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "query_embedding = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.query(vector=query_embedding, top_k=5, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_memories = [match['metadata'] for match in search_results['matches']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'I explained that a binary tree consists of a parent with at most two children and illustrated it with a simple example using names.',\n",
       "  'observation': 'The user asked for an explanation of a binary tree and related it to family trees.',\n",
       "  'result': 'The user expressed understanding, indicating that my explanation was clear and effective.',\n",
       "  'thoughts': 'I realized the user was familiar with family trees, which allowed me to relate the concept of a binary tree to something they understood.'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Format Retrieved Memories\n",
    "context = \"\\n\\n\".join([\n",
    "    f\"Observation: {memory['observation']}\\nThoughts: {memory['thoughts']}\\nAction: {memory['action']}\\nResult: {memory['result']}\"\n",
    "    for memory in retrieved_memories\n",
    "])\n",
    "\n",
    "# Step 2: Construct messages for chat-based model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use past memories if they are relevant.\"},\n",
    "        {\"role\": \"system\", \"content\": f\"Here are some relevant past memories:\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# Step 3: Extract the reply\n",
    "assistant_reply = response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! A binary tree can be a great way to visualize family relationships, especially when considering how each person can have a limited number of direct descendants.\\n\\nIn a binary tree:\\n\\n- Each node represents a person (like a family member).\\n- The top node is the \"root\" of the tree, which could represent the oldest generation, such as a grandparent.\\n- Each person (node) can have at most two children (nodes), which could represent their direct descendants, like a parent having two children.\\n\\nFor example, let\\'s say we have a grandparent named \"Grandma.\" She has two children: \"Aunt\" and \"Uncle.\" In this case, Grandma is the root of the tree, Aunt and Uncle are her children (the left and right children of the root node).\\n\\nIf Aunt has two children, \"Cousin1\" and \"Cousin2,\" they would be the left and right children of Aunt. Uncle, on the other hand, might have one child, \"Cousin3,\" who would be the left child of Uncle, while Uncle has no right child.\\n\\nSo, the structure would look something like this:\\n\\n```\\n        Grandma\\n       /      \\\\\\n     Aunt      Uncle\\n    /   \\\\      /\\nCousin1 Cousin2 Cousin3\\n```\\n\\nIn this way, you can see how each generation is represented in a binary tree format, with each person having at most two direct descendants. This structure helps to visualize family relationships clearly, just like a family tree!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant_reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/nvnkgz9s2nnbd6d3c005qd5m0000gn/T/ipykernel_89216/988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input and assistant output\n",
    "user_input = \"Hello my name is Jonas\"\n",
    "assistant_output = \"Hi Jonas, nice to meet you! How can I assist you today?\"\n",
    "\n",
    "# Save the context\n",
    "memory.save_context({\"input\": user_input}, {\"output\": assistant_output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human asks the AI to explain binary trees. The AI explains that a binary tree is a data structure with nodes that can have up to two children. The human then asks how this relates to family trees, and the AI explains that while both represent hierarchical relationships between nodes, binary trees are based on the order of insertion rather than familial relationships. The human introduces themselves as Jonas and the AI greets them and offers assistance, introducing itself as an AI.\n"
     ]
    }
   ],
   "source": [
    "# Load the current summary\n",
    "summary = memory.load_memory_variables({})\n",
    "print(summary['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Your name is Jonas. Is there anything else you would like to know, Jonas? I am here to assist you.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Engage in a conversation\n",
    "response = conversation.predict(input=\"What is my name?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embedding_model, text_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0e9f74a2-5834-4319-b056-a3d13e186647']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example conversation inputs and outputs\n",
    "user_input = \"Hello, can you explain binary trees?\"\n",
    "assistant_output = \"Certainly! A binary tree is a data structure where each node has at most two children.\"\n",
    "\n",
    "# Save the context to update the summary\n",
    "memory.save_context({\"input\": user_input}, {\"output\": assistant_output})\n",
    "\n",
    "# Retrieve the updated summary\n",
    "summary = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# Generate embedding for the summary\n",
    "summary_embedding = embedding_model.embed_query(summary)\n",
    "\n",
    "# Upsert the summary and its embedding into Pinecone\n",
    "vector_store.add_texts(texts=[summary], embeddings=[summary_embedding])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
